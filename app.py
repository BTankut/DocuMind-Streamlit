import os
import streamlit as st
from typing import List, Dict
from langdetect import detect
from more_itertools import chunked
from pathlib import Path
import re
from datetime import datetime
import time
import PyPDF2
import io
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import torch
from dotenv import load_dotenv
import requests
import chromadb
from chromadb import PersistentClient
from chromadb.config import Settings
from chromadb.utils import embedding_functions

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Dil Ã§evirileri
TRANSLATIONS = {
    "tr": {
        "title": "ðŸ“š DokÃ¼man Arama Sistemi",
        "subtitle": "by BarisTankut",
        "description": "PDF ve TXT formatÄ±ndaki RusÃ§a, TÃ¼rkÃ§e ve Ä°ngilizce dokÃ¼manlarÄ± yÃ¼kleyin ve arama yapÄ±n.",
        "upload_title": "ðŸ“ DokÃ¼man YÃ¼kleme",
        "upload_label": "DokÃ¼manlarÄ± yÃ¼kleyin (PDF/TXT) - RusÃ§a, TÃ¼rkÃ§e, Ä°ngilizce",
        "search_tab": "ðŸ” DokÃ¼man Arama",
        "ai_tab": "ðŸ¤– Yapay Zeka Sohbet",
        "search_input": "ðŸ” Arama yapmak iÃ§in bir kelime veya cÃ¼mle girin:",
        "ai_input": "ðŸ’­ DokÃ¼manlar hakkÄ±nda bir soru sorun:",
        "no_results": "âš ï¸ SonuÃ§ bulunamadÄ±.",
        "results_found": "âœ¨ {} sonuÃ§ bulundu!",
        "result_title": "ðŸ“„ SonuÃ§ {} - {} (ParÃ§a {})",
        "doc_stats": "ðŸ“Š DokÃ¼man boyutu: {} | {} karakter",
        "upload_first": "âš ï¸ Ã–nce dokÃ¼man yÃ¼klemelisiniz!",
        "ai_thinking": "ðŸ¤– Yapay zeka dÃ¼ÅŸÃ¼nÃ¼yor...",
        "error": "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {}"
    },
    "en": {
        "title": "ðŸ“š Document Search System",
        "subtitle": "by BarisTankut",
        "description": "Upload and search documents in Russian, Turkish and English (PDF and TXT format).",
        "upload_title": "ðŸ“ Document Upload",
        "upload_label": "Upload documents (PDF/TXT) - Russian, Turkish, English",
        "search_tab": "ðŸ” Document Search",
        "ai_tab": "ðŸ¤– AI Chat",
        "search_input": "ðŸ” Enter a word or phrase to search:",
        "ai_input": "ðŸ’­ Ask a question about the documents:",
        "no_results": "âš ï¸ No results found.",
        "results_found": "âœ¨ {} results found!",
        "result_title": "ðŸ“„ Result {} - {} (Chunk {})",
        "doc_stats": "ðŸ“Š Document size: {} | {} characters",
        "upload_first": "âš ï¸ Please upload documents first!",
        "ai_thinking": "ðŸ¤– AI is thinking...",
        "error": "Sorry, an error occurred: {}"
    },
    "ru": {
        "title": "ðŸ“š Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° Ð¿Ð¾Ð¸ÑÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²",
        "subtitle": "by BarisTankut",
        "description": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼, Ñ‚ÑƒÑ€ÐµÑ†ÐºÐ¾Ð¼ Ð¸ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐ°Ñ… (Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ PDF Ð¸ TXT).",
        "upload_title": "ðŸ“ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²",
        "upload_label": "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ (PDF/TXT) - Ñ€ÑƒÑÑÐºÐ¸Ð¹, Ñ‚ÑƒÑ€ÐµÑ†ÐºÐ¸Ð¹, Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ð¹",
        "search_tab": "ðŸ” ÐŸÐ¾Ð¸ÑÐº Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²",
        "ai_tab": "ðŸ¤– Ð˜Ð˜ Ñ‡Ð°Ñ‚",
        "search_input": "ðŸ” Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÑÐ»Ð¾Ð²Ð¾ Ð¸Ð»Ð¸ Ñ„Ñ€Ð°Ð·Ñƒ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°:",
        "ai_input": "ðŸ’­ Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ…:",
        "no_results": "âš ï¸ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹.",
        "results_found": "âœ¨ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {} Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²!",
        "result_title": "ðŸ“„ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ {} - {} (Ð§Ð°ÑÑ‚ÑŒ {})",
        "doc_stats": "ðŸ“Š Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°: {} | {} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²",
        "upload_first": "âš ï¸ Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹!",
        "ai_thinking": "ðŸ¤– Ð˜Ð˜ Ð´ÑƒÐ¼Ð°ÐµÑ‚...",
        "error": "Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ°: {}"
    }
}

class DocumentSearchSystem:
    def __init__(self):
        self.documents: List[Dict] = []
        self.doc_dir = "documents"
        self.model = None
        self.embeddings = {}
        
        # DokÃ¼man dizinini oluÅŸtur
        os.makedirs(self.doc_dir, exist_ok=True)
        
        # ChromaDB istemcisini baÅŸlat
        try:
            self.chroma_client = PersistentClient(path="chroma_db")
        except Exception as e:
            st.error(f"ChromaDB baÅŸlatma hatasÄ±: {str(e)}")
            self.chroma_client = None
            return
        
        # Koleksiyonu oluÅŸtur veya var olanÄ± al
        try:
            self.sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
                model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
            )
            
            # Koleksiyonu sil ve yeniden oluÅŸtur
            try:
                self.chroma_client.delete_collection(name="multilingual_documents")
            except:
                pass
                
            self.collection = self.chroma_client.create_collection(
                name="multilingual_documents",
                embedding_function=self.sentence_transformer_ef
            )
        except Exception as e:
            st.error(f"Koleksiyon oluÅŸturma hatasÄ±: {str(e)}")
            self.collection = None
        
        # Model yÃ¼kleniyor mesajÄ±
        if not st.session_state.get('model_loaded', False):
            with st.spinner('ðŸ¤– Yapay zeka modeli yÃ¼kleniyor... (Ä°lk aÃ§Ä±lÄ±ÅŸta biraz zaman alabilir)'):
                self.load_model()
                st.session_state.model_loaded = True
    
    def load_model(self):
        """RusÃ§a dil modelini yÃ¼kle"""
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
    def get_embedding(self, text: str) -> np.ndarray:
        """Metin iÃ§in vektÃ¶r oluÅŸtur"""
        return self.model.encode(text, convert_to_tensor=True)
    
    def compute_similarity(self, query_embedding: torch.Tensor, text_embedding: torch.Tensor) -> float:
        """Benzerlik skorunu hesapla"""
        return torch.nn.functional.cosine_similarity(query_embedding.unsqueeze(0), 
                                                   text_embedding.unsqueeze(0)).item()
    
    def extract_pdf_text(self, file) -> str:
        """PDF dosyasÄ±ndan metin Ã§Ä±kar"""
        try:
            pdf_reader = PyPDF2.PdfReader(file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            st.error(f"âŒ PDF okuma hatasÄ±: {str(e)}")
            return ""
        
    def save_document(self, file) -> bool:
        try:
            # Dosya uzantÄ±sÄ±nÄ± kontrol et
            file_ext = Path(file.name).suffix.lower()
            
            if file_ext == '.pdf':
                content = self.extract_pdf_text(file)
                if not content:
                    return False
            elif file_ext == '.txt':
                content = file.read().decode('utf-8')
            else:
                st.warning(f"âš ï¸ {file.name} desteklenmeyen dosya formatÄ±! Sadece .pdf ve .txt dosyalarÄ± kabul edilir.")
                return False
            
            # Dil kontrolÃ¼
            if not content:
                st.warning("âš ï¸ Bu belge metin iÃ§ermiyor!")
                return False
                
            try:
                detected_lang = detect(content)
                if detected_lang not in ['ru', 'tr', 'en']:
                    st.warning(f"âš ï¸ Bu belge desteklenmeyen bir dilde ({detected_lang})! Sadece RusÃ§a, TÃ¼rkÃ§e ve Ä°ngilizce belgeler kabul edilir.")
                    return False
            except:
                st.warning("âš ï¸ Belgenin dilini tespit edilemedi!")
                return False
            
            # DokÃ¼manÄ± anlamlÄ± bÃ¶lÃ¼mlere ayÄ±r
            def split_by_sections(text):
                # RusÃ§a, Ä°ngilizce ve TÃ¼rkÃ§e bÃ¶lÃ¼m iÅŸaretleri
                section_patterns = [
                    r'Ð Ð°Ð·Ð´ÐµÐ» \d+', r'Ð“Ð»Ð°Ð²Ð° \d+', r'Ð¢Ð¾Ð¼ \d+',
                    r'Section \d+', r'Chapter \d+', r'Volume \d+',
                    r'BÃ¶lÃ¼m \d+', r'KÄ±sÄ±m \d+'
                ]
                
                # Metni Ã¶nce satÄ±rlara bÃ¶l
                lines = text.split('\n')
                sections = []
                current_section = []
                
                for line in lines:
                    # EÄŸer satÄ±r bÃ¶lÃ¼m baÅŸlangÄ±cÄ± ise
                    if any(re.match(pattern, line.strip()) for pattern in section_patterns):
                        if current_section:
                            section_text = '\n'.join(current_section).strip()
                            if section_text:
                                sections.append(section_text)
                            current_section = []
                    current_section.append(line)
                
                # Son bÃ¶lÃ¼mÃ¼ ekle
                if current_section:
                    section_text = '\n'.join(current_section).strip()
                    if section_text:
                        sections.append(section_text)
                
                # EÄŸer bÃ¶lÃ¼m bulunamadÄ±ysa, paragraf bazlÄ± bÃ¶l
                if not sections:
                    sections = [p.strip() for p in text.split('\n\n') if p.strip()]
                
                return sections
            
            # DokÃ¼manÄ± bÃ¶lÃ¼mlere ayÄ±r
            chunks = split_by_sections(content)
            
            try:
                # ChromaDB'ye ekle
                self.collection.add(
                    documents=chunks,
                    metadatas=[{"title": file.name} for _ in chunks],
                    ids=[f"{file.name}_{i}" for i in range(len(chunks))]
                )
            except Exception as e:
                st.error(f"âŒ ChromaDB hatasÄ±: {str(e)}")
                st.error(f"BÃ¶lÃ¼m sayÄ±sÄ±: {len(chunks)}")
                st.error(f"En bÃ¼yÃ¼k bÃ¶lÃ¼m boyutu: {max(len(chunk) for chunk in chunks)} karakter")
                return False
            
            # Bellek iÃ§i dokÃ¼manlara da ekle
            document = {
                'title': file.name,
                'content': content,
                'chunks': chunks,
                'date_added': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            self.documents.append(document)
            
            return True
            
        except Exception as e:
            st.error(f"âŒ Dosya kaydetme hatasÄ±: {str(e)}")
            return False
    
    def search_documents(self, query: str, chunk_size: int = 500) -> List[Dict]:
        """DokÃ¼manlarda arama yap"""
        try:
            # VektÃ¶r aramasÄ±
            vector_results = self.collection.query(
                query_texts=[query],
                n_results=5
            )
            
            # Metin tabanlÄ± arama iÃ§in regex pattern
            pattern = re.compile(query, re.IGNORECASE)
            
            # TÃ¼m dokÃ¼manlarÄ± tara
            text_results = []
            for doc in self.documents:
                for chunk in doc['chunks']:
                    if pattern.search(chunk):
                        text_results.append({
                            'chunk': chunk,
                            'title': doc['title'],
                            'score': 1.0 if query.lower() in chunk.lower() else 0.8
                        })
            
            # SonuÃ§larÄ± birleÅŸtir
            combined_results = []
            
            # VektÃ¶r sonuÃ§larÄ±nÄ± ekle
            if vector_results['documents']:
                for doc, metadata, score in zip(
                    vector_results['documents'][0],
                    vector_results['metadatas'][0],
                    vector_results['distances'][0]
                ):
                    combined_results.append({
                        'chunk': doc,
                        'title': metadata['title'],
                        'score': 1 - score  # ChromaDB'de dÃ¼ÅŸÃ¼k mesafe = yÃ¼ksek benzerlik
                    })
            
            # Metin sonuÃ§larÄ±nÄ± ekle
            combined_results.extend(text_results)
            
            # Tekrar eden sonuÃ§larÄ± kaldÄ±r ve sÄ±rala
            seen = set()
            unique_results = []
            for result in combined_results:
                if result['chunk'] not in seen:
                    seen.add(result['chunk'])
                    unique_results.append(result)
            
            # Skora gÃ¶re sÄ±rala
            unique_results.sort(key=lambda x: x['score'], reverse=True)
            
            return unique_results[:5]  # En iyi 5 sonucu dÃ¶ndÃ¼r
            
        except Exception as e:
            st.error(f"âŒ Arama hatasÄ±: {str(e)}")
            return []
    
    def highlight_text(self, text: str, query: str) -> str:
        """Metinde arama sorgusunu vurgula"""
        if not query:
            return text
            
        pattern = re.compile(f'({re.escape(query)})', re.IGNORECASE)
        return pattern.sub(r'**\1**', text)
            
    def ask_ai(self, question: str, context: str) -> str:
        """GPT-4'e soru sor"""
        try:
            api_key = os.getenv('OPENROUTER_API_KEY')
            if not api_key:
                return "API anahtarÄ± bulunamadÄ±. LÃ¼tfen .env dosyasÄ±nÄ± kontrol edin."

            headers = {
                "Authorization": f"Bearer {api_key}",
                "HTTP-Referer": "https://github.com/BTankut/DocuMind-Streamlit",
                "X-Title": "DocuMind-Streamlit",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "openai/gpt-4",
                "messages": [
                    {
                        "role": "system",
                        "content": """Sen Ã§ok yetenekli bir RusÃ§a dokÃ¼man analiz asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki Ã¶zelliklere sahipsin:

1. Dil Yetenekleri:
   - RusÃ§a metinleri mÃ¼kemmel ÅŸekilde anlama ve analiz etme
   - RusÃ§a-TÃ¼rkÃ§e Ã§eviri yapabilme
   - KullanÄ±cÄ±nÄ±n tercih ettiÄŸi dilde yanÄ±t verme
   - Teknik ve akademik RusÃ§a terminolojiye hakimiyet

2. Analiz Yetenekleri:
   - DokÃ¼manlarÄ±n ana fikrini Ã§Ä±karma
   - Ã–nemli noktalarÄ± Ã¶zetleme
   - Metindeki anahtar kavramlarÄ± belirleme
   - BaÄŸlamsal iliÅŸkileri kurma
   - KarmaÅŸÄ±k fikirleri basitleÅŸtirme

3. Ä°letiÅŸim TarzÄ±:
   - Net ve anlaÅŸÄ±lÄ±r ifadeler kullanma
   - GerektiÄŸinde detaylÄ± aÃ§Ä±klamalar yapma
   - Profesyonel ve saygÄ±lÄ± bir ton kullanma
   - KullanÄ±cÄ± sorularÄ±nÄ± doÄŸru yorumlama

4. Ã–zel Yetenekler:
   - RusÃ§a dokÃ¼manlardan alÄ±ntÄ± yapabilme
   - Teknik terimleri aÃ§Ä±klayabilme
   - Metinler arasÄ± baÄŸlantÄ±lar kurabilme
   - GerektiÄŸinde ek kaynaklara yÃ¶nlendirme

Verilen baÄŸlamÄ± kullanarak sorularÄ± bu yetenekler Ã§erÃ§evesinde yanÄ±tla. Her zaman doÄŸru, gÃ¼venilir ve yapÄ±cÄ± bilgiler sun."""
                    },
                    {
                        "role": "user",
                        "content": f"""BaÄŸlam:
{context}

Soru:
{question}

LÃ¼tfen yukarÄ±daki yeteneklerini kullanarak bu soruyu yanÄ±tla."""
                    }
                ]
            }
            
            response = requests.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json=data
            )
            
            if response.status_code == 200:
                return response.json()['choices'][0]['message']['content']
            else:
                return f"API hatasÄ±: {response.status_code} - {response.text}"
            
        except Exception as e:
            return f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}"

def format_size(size_bytes: int) -> str:
    """Boyutu okunabilir formata Ã§evir"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} TB"

def main():
    # Session state'i baÅŸlat
    if 'messages' not in st.session_state:
        st.session_state.messages = []
        
    # Session state baÅŸlatma
    if "lang" not in st.session_state:
        st.session_state.lang = "tr"
        
    # Sidebar ayarlarÄ±
    with st.sidebar:
        # Dil seÃ§imi
        lang = st.selectbox(
            "ðŸŒ Dil SeÃ§imi",
            ["TÃ¼rkÃ§e", "English", "Ð ÑƒÑÑÐºÐ¸Ð¹"],
            index=["tr", "en", "ru"].index(st.session_state.lang)
        )
        st.session_state.lang = {"TÃ¼rkÃ§e": "tr", "English": "en", "Ð ÑƒÑÑÐºÐ¸Ð¹": "ru"}[lang]
    
    # Ã‡evirileri al
    t = TRANSLATIONS[st.session_state.lang]
    
    # BaÅŸlÄ±k ve alt baÅŸlÄ±k
    st.markdown(f"""
        <h1 style='font-size: 2.3em; margin-bottom: 0;'>{t["title"]}</h1>
        <p style='font-size: 0.8em; color: gray; margin-top: 0; margin-bottom: 20px;'>{t["subtitle"]}</p>
        """, unsafe_allow_html=True)
    st.write(t["description"])
    
    system = DocumentSearchSystem()
    
    # Sol sidebar
    with st.sidebar:
        st.header(t["upload_title"])
        uploaded_files = st.file_uploader(
            t["upload_label"],
            type=["txt", "pdf"],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            for file in uploaded_files:
                system.save_document(file)
    
    # Ana iÃ§erik
    tab1, tab2 = st.tabs([t["search_tab"], t["ai_tab"]])
    
    # Arama sekmesi
    with tab1:
        query = st.text_input(t["search_input"])
        
        if query:
            results = system.search_documents(query)
            
            if not results:
                st.warning(t["no_results"])
            else:
                st.success(t["results_found"].format(len(results)))
                
                for i, result in enumerate(results, 1):
                    with st.expander(
                        t["result_title"].format(i, result['title'], 1)
                    ):
                        st.markdown(f"""
                        {result['chunk']}
                        
                        ---
                        {t["doc_stats"].format(format_size(len(result['chunk'].encode('utf-8'))), len(result['chunk']))}
                        """)
    
    # Yapay Zeka sekmesi
    with tab2:
        if not system.documents:
            st.warning(t["upload_first"])
        else:
            # Chat arayÃ¼zÃ¼
            st.markdown("""
                <style>
                /* Ana container */
                .main {
                    max-width: 800px;
                    margin: 0 auto;
                    padding: 0;
                }

                /* Chat container */
                .chat-container {
                    display: flex;
                    flex-direction: column;
                    gap: 0;
                    padding-bottom: 120px;
                }

                /* Mesaj stilleri */
                .message {
                    display: flex;
                    padding: 20px 0;
                    margin: 0;
                    border-bottom: 1px solid rgba(0,0,0,0.1);
                }

                .message-content {
                    max-width: 800px;
                    margin: 0 auto;
                    width: 100%;
                    padding: 0 20px;
                }

                .user-message {
                    background: #f7f7f8;
                }

                .assistant-message {
                    background: #ffffff;
                }

                /* Input container */
                .input-container {
                    position: fixed;
                    bottom: 0;
                    left: 0;
                    right: 0;
                    background: linear-gradient(180deg, transparent 0%, #f7f7f8 50%);
                    padding: 2rem 1rem;
                }

                .input-box {
                    max-width: 800px;
                    margin: 0 auto;
                    background: white;
                    border: 1px solid #e5e5e5;
                    border-radius: 0.5rem;
                    box-shadow: 0 2px 6px rgba(0,0,0,.05);
                    padding: 0.5rem;
                }


                /* Chat input Ã¶zelleÅŸtirmeleri */
                section[data-testid="stChatInput"] {
                    position: fixed !important;
                    bottom: 0 !important;
                    left: 0 !important;
                    right: 0 !important;
                    padding: 2rem 5rem !important;
                    background: linear-gradient(180deg, transparent 0%, var(--background-color) 50%) !important;
                    z-index: 1000 !important;
                }

                section[data-testid="stChatInput"] > div {
                    max-width: 48rem !important;
                    margin: 0 auto !important;
                    background: var(--background-color) !important;
                    border: 1px solid rgba(0,0,0,0.1) !important;
                    border-radius: 1rem !important;
                    box-shadow: 0 0 15px rgba(0,0,0,0.1) !important;
                    padding: 0.75rem !important;
                }

                section[data-testid="stChatInput"] textarea {
                    background: transparent !important;
                    border: none !important;
                    padding: 0.5rem !important;
                    resize: none !important;
                    height: 24px !important;
                    overflow-y: hidden !important;
                }

                [data-theme="dark"] section[data-testid="stChatInput"] {
                    background: linear-gradient(180deg, transparent 0%, #2d2d2d 50%) !important;
                }

                [data-theme="dark"] section[data-testid="stChatInput"] > div {
                    background: #40414f !important;
                    border-color: #565869 !important;
                }

                [data-theme="dark"] section[data-testid="stChatInput"] textarea {
                    color: #ffffff !important;
                }
                </style>
            """, unsafe_allow_html=True)
            
            # Chat container
            st.markdown('<div class="chat-container">', unsafe_allow_html=True)
            
            # MesajlarÄ± gÃ¶ster
            for message in st.session_state.messages:
                message_class = "message " + ("user-message" if message["role"] == "user" else "assistant-message")
                st.markdown(f"""
                    <div class="{message_class}">
                        <div class="message-content">
                            {message["content"]}
                        </div>
                    </div>
                """, unsafe_allow_html=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            # Input alanÄ±
            if question := st.chat_input(t["ai_input"]):
                # TÃ¼m dokÃ¼manlarÄ± birleÅŸtir
                all_docs = "\n---\n".join([
                    f"Document: {doc['title']}\nContent: {doc['content'][:1000]}"
                    for doc in system.documents
                ])
                
                # MesajlarÄ± ekle
                st.session_state.messages.append({"role": "user", "content": question})
                
                # AI yanÄ±tÄ±
                with st.spinner(t["ai_thinking"]):
                    answer = system.ask_ai(question, all_docs)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # SayfayÄ± yenile
                st.rerun()

if __name__ == "__main__":
    main()
